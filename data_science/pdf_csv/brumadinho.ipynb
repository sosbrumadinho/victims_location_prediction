{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformando PDF para Dataframe (Brumadinho)\n",
    "Neste projeto é aplicado técnicas de OCR para transformar o PDF disponibilizado pela vale em um dataframe do pandas, que também pode ser salvo em CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalando requisitos\n",
    "# !apt update -y && apt upgrade -y\n",
    "# !apt install -y imagemagick tesseract-ocr libtesseract libtesseract-dev\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotacas\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from os import walk\n",
    "from natsort import natsorted, ns\n",
    "from PIL import Image\n",
    "import PIL.ImageOps  \n",
    "import pytesseract\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "mpl.rcParams['figure.figsize'] = (15,15)\n",
    "\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para definir onde imagem deve ser cortada\n",
    "def get_crop_size(path):\n",
    "    page_img = Image.open(path)\n",
    "\n",
    "    top = 0\n",
    "    left = 0\n",
    "    right = page_img.size[0]\n",
    "    bottom = page_img.size[1]\n",
    "    \n",
    "    #finding top\n",
    "    for i in range(0,500):\n",
    "        pixel = page_img.getpixel((i,i))\n",
    "        pixel_mean = np.mean(pixel)\n",
    "        if pixel_mean < 200 and top == 0:\n",
    "            top = i\n",
    "\n",
    "    # finding left\n",
    "    for i in range(0,500):\n",
    "        pixel = page_img.getpixel((i,top))\n",
    "        pixel_mean = np.mean(pixel)\n",
    "        if pixel_mean < 240 and left == 0:\n",
    "            left = i\n",
    "\n",
    "    return (left,top + 220,left + 1565,top + 3000)\n",
    "\n",
    "# função para recuperar dados da página\n",
    "# fonte: https://github.com/dieegom/brumadinho_location/blob/master/crawler/crawler.py\n",
    "\n",
    "def get_raw_data():\n",
    "    url = \"http://brumadinho.vale.com/listagem-pessoas-sem-contato.html\"\n",
    "    r = requests.get(url)\n",
    "    r.encoding = 'utf-8'\n",
    "    return r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo cconstantes\n",
    "\n",
    "categoryList = ['Terceiro/Comunidade','Próprios']\n",
    "statusList = ['Localizado','Sem contato','Óbito confirmado pelo IML', 'Alta']\n",
    "\n",
    "doc_upper = 0\n",
    "doc_lower = 3260\n",
    "\n",
    "path = \"pages\"\n",
    "# path = \"/tmp/pages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperando endereço do PDF a partir da página a vale\n",
    "html = get_raw_data()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "pdf = ''\n",
    "for a in soup.find_all('a', href=True):\n",
    "    href = a['href']\n",
    "    if href.endswith('pdf'):\n",
    "        pdf = href\n",
    "\n",
    "filename = pdf.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo PDF para PNG\n",
    "# é importante comentar a linha <policy domain=\"coder\" rights=\"none\" pattern=\"PDF\" /> em /etc/ImageMagick-6/policy.xml\n",
    "\n",
    "!mkdir $path\n",
    "!convert -density 300 $pdf $path/page.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando lista de páginas\n",
    "\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(path):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "\n",
    "pages = natsorted(f, key=lambda y: y.lower())\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando DataSet\n",
    "\n",
    "dataset = {\n",
    "    'nomes':[],\n",
    "    'categorias':[],\n",
    "    'status':[]\n",
    "}\n",
    "\n",
    "for page in pages:\n",
    "    crop = get_crop_size(\"{}/{}\".format(path,page))\n",
    "    page_img = Image.open(\"{}/{}\".format(path,page))\n",
    "    page_img = page_img.crop(crop)\n",
    "    \n",
    "    segments = {\n",
    "        'nomes':      (0,0,775,page_img.size[1]),\n",
    "        'categorias': (775,0,1150,page_img.size[1]),\n",
    "        'status':     (1150,0,page_img.size[0],page_img.size[1]),\n",
    "    }\n",
    "\n",
    "#     page_img = Image.open(\"{}/{}\".format(path,page))\n",
    "    for seg_name in segments.keys():\n",
    "        image = page_img.crop(segments[seg_name])\n",
    "        \n",
    "        image_text = pytesseract.image_to_string(image, lang=\"por\")\n",
    "        image_text_list = image_text.split('\\n')\n",
    "        image_text_list = list(filter(lambda x: len(x.strip()) > 0, image_text_list))\n",
    "        \n",
    "        if seg_name != 'topo':\n",
    "            dataset[seg_name] += image_text_list\n",
    "            print(page, seg_name, len(image_text_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legenda\n",
    "\n",
    "**categorias_2**\n",
    "\n",
    "1. Terceiro/Comunidade\n",
    "2. Próprios\n",
    "\n",
    "**status_2**\n",
    "\n",
    "1. Localizado\n",
    "2. Sem contato\n",
    "3. Óbito confirmado pelo IML\n",
    "4. Alta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando DataFrame\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df['categorias_2'] = [categoryList.index(process.extractOne(test_word, categoryList)[0]) +1 for test_word in df['categorias'].values]\n",
    "df['status_2'] = [ statusList.index(process.extractOne(test_word, statusList)[0]) + 1 for test_word in df['status'].values]\n",
    "df.to_csv(filename.replace('pdf','csv'),index = False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
